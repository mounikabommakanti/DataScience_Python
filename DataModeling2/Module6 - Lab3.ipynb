{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT210x - Programming with Python for DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module6- Lab3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is intentionally missing! Read the directions on the course lab page!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the /Module6/Datasets/parkinsons.data data set into a variable X, being sure to drop the name column.\n",
    "Splice out the status column into a variable y and delete it from X.\n",
    "Perform a train/test split. 30% test group size, with a random_state equal to 7.\n",
    "Create a SVC classifier. Don't specify any parameters, just leave everything as default. Fit it against your training data and then score your testing data.\n",
    "What accuracy did you score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'name', u'MDVP:Fo(Hz)', u'MDVP:Fhi(Hz)', u'MDVP:Flo(Hz)',\n",
       "       u'MDVP:Jitter(%)', u'MDVP:Jitter(Abs)', u'MDVP:RAP', u'MDVP:PPQ',\n",
       "       u'Jitter:DDP', u'MDVP:Shimmer', u'MDVP:Shimmer(dB)', u'Shimmer:APQ3',\n",
       "       u'Shimmer:APQ5', u'MDVP:APQ', u'Shimmer:DDA', u'NHR', u'HNR', u'status',\n",
       "       u'RPDE', u'DFA', u'spread1', u'spread2', u'D2', u'PPE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=pd.read_csv('Datasets/parkinsons.data')\n",
    "#X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MDVP:Fo(Hz)         195\n",
       "MDVP:Fhi(Hz)        195\n",
       "MDVP:Flo(Hz)        195\n",
       "MDVP:Jitter(%)      195\n",
       "MDVP:Jitter(Abs)    195\n",
       "MDVP:RAP            195\n",
       "MDVP:PPQ            195\n",
       "Jitter:DDP          195\n",
       "MDVP:Shimmer        195\n",
       "MDVP:Shimmer(dB)    195\n",
       "Shimmer:APQ3        195\n",
       "Shimmer:APQ5        195\n",
       "MDVP:APQ            195\n",
       "Shimmer:DDA         195\n",
       "NHR                 195\n",
       "HNR                 195\n",
       "RPDE                195\n",
       "DFA                 195\n",
       "spread1             195\n",
       "spread2             195\n",
       "D2                  195\n",
       "PPE                 195\n",
       "dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=X[['status']]\n",
    "X.drop(y,axis=1,inplace=True)\n",
    "X.drop('name',axis=1,inplace=True)\n",
    "X.dropna(axis=0,inplace=True)\n",
    "X.count()\n",
    "#X.dtypes\n",
    "#y.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.30, random_state=7)\n",
    "#y_train=y_train.reshape(1,-1)\n",
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc= SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81355932203389836"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That accuracy was just too low to be useful. We need to get it up. One way you could go about doing that would be to manually try a bunch of combinations of C, and gamma values for your rbf kernel. But that could literally take forever. Also, you might unknowingly skip a pair of values that would have resulted in a very good accuracy.\n",
    "Instead, lets get the computer to do what computers do best. Program a naive, best-parameter search by creating nested for-loops. The outer for-loop should iterate a variable C from 0.05 to 2, using 0.05 unit increments. The inner for-loop should increment a variable gamma from 0.001 to 0.1, using 0.001 unit increments. As you know, Python ranges won't allow for float intervals, so you'll have to do some research on NumPy ARanges, if you don't already know how to use them.\n",
    "Since the goal is to find the parameters that result in the model having the best accuracy score, you'll need a best_score = 0 variable that you initialize outside of the for-loops. Inside the inner for-loop, create an SVC model and pass in the C and gamma parameters its class constructor. Train and score the model appropriately. If the current best_score is less than the model's score, update the best_score being sure to print it out, along with the C and gamma values that resulted in it.\n",
    "\n",
    "After running your assignment again, what is the highest accuracy score you are able to get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bestscore', 0.93220338983050843, 'C=', 1.9500000000000002, 'gamma=', 0.099000000000000005)\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "for C in np.arange(0.05,2,0.05):\n",
    "        for gamma in np.arange(0.001,0.1,0.001):\n",
    "            #X=preprocessing.StandardScaler().fit_transform(X)\n",
    "            X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.30, random_state=7)\n",
    "           \n",
    "            #X_test=preprocessing.Normalizer().fit_transform(X_test)\n",
    "           \n",
    "            svc= SVC(C=C,gamma=gamma)\n",
    "            svc.fit(X_train,y_train)\n",
    "            score=svc.score(X_test,y_test)\n",
    "            if(best_score<score):\n",
    "                best_score=score\n",
    "                \n",
    "print('bestscore',best_score,'C=',C,'gamma=',gamma)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait a second. Pull open the dataset's label file from: https://archive.ics.uci.edu/ml/datasets/Parkinsons\n",
    "Look at the units on those columns: Hz, %, Abs, dB, etc. What happened to transforming your data? With all of those units interacting with one another, some pre-processing is surely in order.\n",
    "Right after you preform the train/test split but before you train your model, inject SciKit-Learn's pre-processing code. Unless you have a good idea which one is going to work best, you're going to have to try the various pre-processors one at a time, checking to see if they improve your predictive accuracy.\n",
    "Experiment with Normalizer(), MaxAbsScaler(), MinMaxScaler(), KernelCenterer(), and StandardScaler().\n",
    "\n",
    "After trying all of these scalers, what is the new highest accuracy score you're able to achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bestscore', 0.93220338983050843, 'C=', 1.9500000000000002, 'gamma=', 0.099000000000000005)\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "for C in np.arange(0.05,2,0.05):\n",
    "        for gamma in np.arange(0.001,0.1,0.001):\n",
    "            X=preprocessing.StandardScaler().fit_transform(X)\n",
    "            X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.30, random_state=7)\n",
    "           \n",
    "            #X_test=preprocessing.Normalizer().fit_transform(X_test)\n",
    "           \n",
    "            svc= SVC(C=C,gamma=gamma)\n",
    "            svc.fit(X_train,y_train)\n",
    "            score=svc.score(X_test,y_test)\n",
    "            if(best_score<score):\n",
    "                best_score=score\n",
    "                \n",
    "print('bestscore',best_score,'C=',C,'gamma=',gamma)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score keeps creeping upwards. Let's have one more go at it. Remember how in a previous lab we discovered that SVM's are a bit sensitive to outliers and that just throwing all of our unfiltered, dirty or noisy data at it, particularly in high-dimensionality space, can actually cause the accuracy score to suffer?\n",
    "Well, let's try to get rid of some useless features. Immediately after you do the pre-processing, run PCA on your dataset. The original dataset has 22 columns and 1 label column. So try experimenting with PCA n_component values between 4 and 14. Are you able to get a better accuracy?\n",
    "If you are not, then forget about PCA entirely. However if you are able to get a higher score, then be sure keep that accuracy score in mind, and comment out all the PCA code for now.\n",
    "In the same spot, run Isomap on the data. Manually experiment with every inclusive combination of n_neighbors between 2 and 5, and n_components between 4 and 6. Are you able to get a better accuracy?\n",
    "If you are not, then forget about isomap entirely. However if you are able to get a higher score, then be sure keep that figure in mind.\n",
    "If either PCA or Isomap helped you out, then uncomment out the appropriate transformation code so that you have the highest accuracy possible.\n",
    "\n",
    "What is your highest accuracy score on this assignment to date?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def doPCA(data, dimensions=7):\n",
    "    model = PCA(n_components=dimensions, svd_solver='randomized', random_state=7)\n",
    "    model.fit(data)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bestscore', 0.93220338983050843, 'C=', 1.9500000000000002, 'gamma=', 0.099000000000000005)\n"
     ]
    }
   ],
   "source": [
    "T = preprocessing.StandardScaler().fit_transform(X)\n",
    "display_pca = doPCA(T)\n",
    "X_pca = display_pca.transform(T)\n",
    "#CC = display_pca.transform(centroids)\n",
    "best_score = 0\n",
    "for C in np.arange(0.05,2,0.05):\n",
    "        for gamma in np.arange(0.001,0.1,0.001):\n",
    "            \n",
    "            X_train,X_test,y_train,y_test = train_test_split(X_pca,y, test_size=0.30, random_state=7)\n",
    "            #X_train=preprocessing.Normalizer().fit_transform(X_train)\n",
    "            #X_test=preprocessing.Normalizer().fit_transform(X_test)\n",
    "           \n",
    "            svc= SVC(C=C,gamma=gamma)\n",
    "            svc.fit(X_train,y_train)\n",
    "            score=svc.score(X_test,y_test)\n",
    "            if(best_score<score):\n",
    "                best_score=score\n",
    "                \n",
    "print('bestscore',best_score,'C=',C,'gamma=',gamma) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bestscore', 0.96610169491525422, 'C=', 1.9500000000000002, 'gamma=', 0.099000000000000005)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import manifold\n",
    "T = preprocessing.StandardScaler().fit_transform(X)\n",
    "for n_neighbors in range(2,5):\n",
    "    for n_components in range(4,6):\n",
    "    \n",
    "        iso = manifold.Isomap(n_neighbors=n_neighbors, n_components=n_components)\n",
    "        iso=iso.fit(X)\n",
    "        X_iso=iso.transform(X)\n",
    "        #manifold = iso.transform(X)\n",
    "        best_score = 0\n",
    "        for C in np.arange(0.05,2,0.05):\n",
    "            for gamma in np.arange(0.001,0.1,0.001):\n",
    "            \n",
    "                X_train,X_test,y_train,y_test = train_test_split(X_iso,y, test_size=0.30, random_state=7)\n",
    "            #X_train=preprocessing.Normalizer().fit_transform(X_train)\n",
    "            #X_test=preprocessing.Normalizer().fit_transform(X_test)\n",
    "           \n",
    "                svc= SVC(C=C,gamma=gamma)\n",
    "                svc.fit(X_train,y_train)\n",
    "                score=svc.score(X_test,y_test)\n",
    "                if(best_score<score):\n",
    "                    best_score=score\n",
    "                \n",
    "print('bestscore',best_score,'C=',C,'gamma=',gamma) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "58px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
